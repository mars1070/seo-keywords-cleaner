import streamlit as st
import pandas as pd
import os
from pathlib import Path
from dotenv import load_dotenv
from openai import OpenAI
import tempfile
import zipfile
import io
from datetime import datetime
from tqdm import tqdm
from thefuzz import fuzz
from collections import defaultdict
import time

# R√©cup√©ration de la cl√© API depuis les secrets Streamlit
if 'OPENAI_API_KEY' in st.secrets:
    openai_api_key = st.secrets['OPENAI_API_KEY']
    os.environ["OPENAI_API_KEY"] = openai_api_key
else:
    st.error("‚ö†Ô∏è La cl√© API OpenAI n'est pas configur√©e. Veuillez la configurer dans les secrets Streamlit.")
    st.stop()

# Initialisation du client OpenAI
client = OpenAI(
    api_key=os.environ["OPENAI_API_KEY"],
)

# Constantes
MIN_VOLUME = 20  # Volume minimum fix√© √† 20
MAX_KEYWORDS = 2500  # Maximum de mots-cl√©s fix√© √† 2500
BATCH_SIZE = 100

# Liste des mots-cl√©s √† exclure
EXCLUDED_WORDS = {
    # Marques d'√©lectronique et √©lectrom√©nager
    'philips', 'phillips', 'samsung', 'apple', 'sony', 'lg', 'panasonic', 'xiaomi', 'huawei',
    'bosch', 'siemens', 'whirlpool', 'miele', 'dyson', 'rowenta', 'moulinex', 'seb', 'krups',
    'delonghi', 'kitchenaid', 'kenwood', 'braun', 'tefal', 'beko', 'smeg', 'brandt', 'candy',
    'hoover', 'electrolux', 'aeg', 'sharp', 'toshiba', 'lenovo', 'asus', 'acer', 'hp', 'dell',
    
    # Marques de caf√© et th√©
    'nespresso', 'senseo', 'tassimo', 'dolce gusto', 'lavazza', 'lipton', 'kusmi', 'mariage freres',
    'malongo', 'carte noire', 'l or', 'illy', 'segafredo', 'maxwell', 'twinings', 'tetley',
    
    # Marques d'alimentation
    'nestle', 'danone', 'coca cola', 'pepsi', 'ferrero', 'kelloggs', 'mars', 'heinz', 'barilla',
    'bonne maman', 'president', 'lustucru', 'panzani', 'fleury michon', 'herta', 'bonduelle',
    
    # Marques de mode et beaut√©
    'nike', 'adidas', 'puma', 'reebok', 'under armour', 'asics', 'new balance', 'converse',
    'zara', 'h&m', 'uniqlo', 'levis', 'gap', 'ralph lauren', 'tommy hilfiger', 'lacoste',
    'loreal', 'maybelline', 'nivea', 'garnier', 'dove', 'olay', 'clinique', 'lancome',
    
    # Marques enfants et jouets
    'lego', 'playmobil', 'fisher price', 'vtech', 'chicco', 'babybjorn', 'hasbro', 'mattel',
    'nerf', 'barbie', 'pokemon', 'bandai', 'melissa doug', 'sophie la girafe',

    # Termes commerciaux
    'pas cher', 'bon march√©', 'petit prix', 'meilleur prix', 'prix bas', 'discount', 'promo',
    'promotion', 'solde', 'soldes', 'destockage', 'destock', 'outlet', 'bon plan', 'bons plans',
    'moins cher', '√©conomique', 'economique', 'prix cass√©', 'prix casse', 'braderie', 'remise',
    'reduction', 'r√©duction', 'offre', 'offres', 'deal', 'deals', 'vente flash', 'black friday',
    'cyber monday', 'french days',

    # Marketplaces et sites d'achat
    'amazon', 'ebay', 'aliexpress', 'walmart', 'target', 'etsy', 'rakuten', 'costco', 'bestbuy',
    'cdiscount', 'fnac', 'darty', 'boulanger', 'leroy merlin', 'castorama', 'decathlon',
    'carrefour', 'auchan', 'leclerc', 'intermarch√©', 'lidl', 'aldi',

    # Boutiques US additionnelles
    'bloomingdales', 'saks fifth avenue', 'bergdorf goodman', 'neiman marcus',
    'dillards', 'belk', 'jcpenney', 'lord and taylor', 'barneys', 'anthropologie',
    'free people', 'lululemon', 'athleta', 'fabletics', 'everlane', 'madewell',
    'j crew', 'banana republic', 'ann taylor', 'lane bryant', 'torrid',
    'hot topic', 'zumiez', 'journeys', 'finish line', 'famous footwear', 'dsw',
    'bath and body works', 'yankee candle', 'pier 1', 'crate and barrel',
    'pottery barn', 'west elm', 'williams sonoma', 'sur la table', 'cabelas',
    'bass pro shops', 'rei', 'eastern mountain sports', 'duluth trading',
    'lands end', 'll bean', 'eddie bauer', 'carhartt', 'orvis', 'tractor supply',
    'ace hardware', 'menards', 'harbor freight', 'northern tool', 'rockler',
    'woodcraft', 'michaels', 'joann fabrics', 'hobby lobby', 'blick art',
    'jerry arterial', 'sweetwater', 'musicians friend', 'american musical supply',

    # Boutiques allemandes
    'otto', 'zalando', 'mediamarkt', 'conrad', 'notebooksbilliger',
    'alternate', 'cyberport', 'computeruniverse', 'mindfactory', 'caseking',
    'kaufland', 'real', 'galeria', 'karstadt', 'douglas', 'dm', 'rossmann',
    'mueller', 'deichmann', 'g√∂rtz', 'about you', 'bonprix', 'tchibo', 'lidl',
    'aldi', 'rewe', 'edeka24', 'hornbach', 'obi', 'bauhaus', 'toom', 'hagebau',
    'globus', 'hellweg', 'poco', 'roller', 'xxxlutz', 'moebel', 'home24',
    'westfalia', 'klingel', 'baur', 'schwab', 'neckermann', 'weltbild',
    'thalia', 'hugendubel', 'buecher', 'medimops', 'rebuy', 'momox',
    'sport scheck', 'bike discount', 'bike components', 'fahrrad',
    'boc24', 'breuninger', 'peek und cloppenburg', 'c&a', 'ernsting family',
    'kik', 'takko', 'new yorker', 'snipes', 'foot locker', 'sidestep',
    'engelhorn', 'sportscheck', 'intersport', 'decathlon', 'pearl', 'voelkner',
    'reichelt', 'digitalo', 'pollin', 'thomann', 'music store', 'justmusic',
    'session', 'zooplus', 'fressnapf', 'futterhaus', 'medpets', 'bitiba',
    'tiierisch', 'hundeland', 'petshop',

    # Mots interrogatifs et recherche
    'how', 'what', 'where', 'when', 'why', 'which', 'who', 'whose', 'whom',
    'comment', 'pourquoi', 'quand', 'quel', 'quelle', 'quoi', 'o√π', 'combien', 'est ce que',
    'can i', 'should i', 'could i', 'would', 'does', 'do i need', 'what if',
    'puis je', 'devrais je', 'est il', 'faut il', 'peut on',

    # Termes commerciaux et prix
    'sale', 'discount', 'deal', 'cheap', 'free', 'best', 'buy', 'price', 'cost', 'worth',
    'solde', 'promo', 'promotion', 'prix', 'pas cher', 'gratuit', 'meilleur prix', 'bon plan',
    'clearance', 'bargain', 'offer', 'coupon', 'voucher', 'rebate', 'saving',
    'destockage', 'remise', 'reduction', 'moins cher', 'economique', 'bon march√©',

    # Termes de recherche et comparaison
    'vs', 'versus', 'review', 'compare', 'comparison', 'difference between', 'better than',
    'avis', 'comparatif', 'test', 'diff√©rence entre', 'mieux que', 'plut√¥t que',
    'rating', 'top', 'best', 'worst', 'ranking', 'rated', 'recommended',
    'classement', 'meilleur', 'pire', 'recommand√©', 'conseill√©',

    # Termes informatifs et tutoriels
    'guide', 'tutorial', 'tuto', 'how to', 'tips', 'advice', 'help', 'instruction', 'manual',
    'guide', 'tutoriel', 'conseil', 'astuce', 'aide', 'mode d emploi', 'notice',
    'learn', 'explain', 'understand', 'meaning', 'definition', 'example', 'difference',
    'apprendre', 'expliquer', 'pattern', 'signification', 'd√©finition', 'exemple',

    # Termes g√©ographiques et localisation
    'near me', 'nearby', 'location', 'store', 'shop', 'where to buy', 'where to find',
    'pr√®s de moi', 'pres de moi', 'magasin', 'boutique', 'o√π acheter', 'ou acheter',
    'o√π trouver', 'ou trouver', 'stock', 'disponible', 'disponibilit√©', 'disponibilite',

    # Questions et recherches
    'what is', 'how to', 'when to', 'where to', 'why is', 'can i', 'should i',
    'quest ce que', 'qu est ce que', 'comment', 'quand', 'o√π', 'ou', 'pourquoi',
    'puis je', 'dois je', 'faut il',

    # Termes temporels
    'today', 'tonight', 'tomorrow', 'yesterday', 'now', 'current', 'latest',
    'aujourd hui', 'ce soir', 'demain', 'hier', 'maintenant', 'actuel', 'dernier',
    'nouvelle collection', 'new collection', 'new arrival', 'nouveaut√©', 'nouveaute',

    # Termes de contrefa√ßon
    'replica', 'replique', 'r√©plique', 'fake', 'faux', 'copie', 'imitation', 
    'contrefa√ßon', 'contrefacon', 'pas original', 'non original',

    # Autres termes non-produit
    'pdf', 'download', 't√©l√©charger', 'telecharger', 'gratuit', 'free', 'sample',
    '√©chantillon', 'echantillon', 'win', 'gagner', 'contest', 'concours',
    'sale', 'vente', 'liquidation', 'destockage', 'destock', 'occasion', 'used',
    'second hand', 'seconde main', 'reconditionn√©', 'reconditionne',
    

}

def contains_excluded_word(keyword):
    """
    V√©rifie si le mot-cl√© contient un des mots exclus
    """
    keyword_lower = keyword.lower()
    return any(word in keyword_lower.split() for word in EXCLUDED_WORDS)

def find_similar_keywords(keywords_df):
    """
    Version simplifi√©e : compare juste les mots tri√©s
    """
    # Cr√©e une colonne avec les mots tri√©s
    keywords_df['Sorted_Words'] = keywords_df['Keyword'].apply(
        lambda x: ' '.join(sorted(x.lower().split()))
    )
    
    # Groupe par mots tri√©s et garde celui avec le plus gros volume
    grouped = keywords_df.groupby('Sorted_Words').agg({
        'Keyword': 'first',  # Garde le premier mot-cl√©
        'Volume': 'sum',     # Additionne les volumes
    }).reset_index()
    
    # Formate le r√©sultat
    return pd.DataFrame({
        'Keyword': grouped['Keyword'],
        'Volume': grouped['Volume'],
        'Original_Keywords': 1
    })

def clean_keywords_with_gpt(keywords_batch, current_batch_display, token_counter):
    try:
        # Afficher uniquement la liste des mots-cl√©s
        current_batch_display.text("\n".join(keywords_batch))

        system_prompt = """Tu es un expert en SEO sp√©cialis√© dans l'optimisation de mots-cl√©s pour le dropshipping sans marque. Ta mission est de nettoyer une liste de mots-cl√©s en respectant ces r√®gles STRICTES :

1. Suppression des mots-cl√©s interdits :
Supprime tout mot-cl√© contenant :

Des noms de marques (ex : Nike, Samsung, Apple, Xiaomi).
Des noms de licences (ex : Disney, Marvel, Star Wars).
Des noms de marketplaces (ex : Amazon, eBay, AliExpress, Temu).
Des mots li√©s aux prix et promotions (ex : pas cher, gratuit, offre sp√©ciale).
Des termes informationnels (ex : avis, review, tutoriel, guide).
Plus de 6 mots dans un mot-cl√©.

2. √âlimination des doublons et optimisation SEO :
Garde uniquement la meilleure version des mots-cl√©s similaires.
Choisis la formulation la plus naturelle (ex : "Table de Jardin" au lieu de "Table Jardin").
Privil√©gie le singulier sauf si le pluriel est plus naturel.
Corrige l'ordre des mots si n√©cessaire pour respecter la langue.

3. Respect strict de la langue :
Ne traduis JAMAIS les mots-cl√©s. Garde-les strictement dans leur langue d'origine.
N'ajoute ni ne modifie aucun mot-cl√©.

4. R√àGLE ABSOLUE des Majuscules :
- CHAQUE mot significatif DOIT commencer par une majuscule :
  * Noms : "Table", "Chaise", "Meuble"
  * Adjectifs : "Grand", "Petit", "Rouge"
  * Mat√©riaux : "Bois", "Verre", "M√©tal"
  * Couleurs : "Bleu", "Noir", "Blanc"
- Les mots de liaison restent en minuscules :
  * Articles : "le", "la", "les"
  * Pr√©positions : "de", "du", "des", "en", "√†"
  * Conjonctions : "et", "ou"

5. Organisation des r√©sultats :
Le premier mot-cl√© retourn√© doit √™tre celui correspondant au terme principal de la cat√©gorie analys√©e (exemple : si la liste contient "Plush Shark", alors "Plush Shark" doit √™tre le premier r√©sultat).
Retourne uniquement la liste finale, un mot-cl√© par ligne, sans commentaires ni num√©rotation."""

        prompt = f"Tu es un expert du Dropshipping SEO et e-commerce sans marques. Analyse ce lot de mots-cl√©s en respectant les r√®gles suivantes :\n" + "\n".join(keywords_batch)
        
        # Ajouter une pause plus longue pour GPT
        time.sleep(2)
        
        try:
            # Appel √† l'API OpenAI
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3
            )

            # Mettre √† jour le compteur de tokens
            token_counter['input_tokens'] += response.usage.prompt_tokens
            token_counter['output_tokens'] += response.usage.completion_tokens
            token_counter['total_tokens'] += response.usage.total_tokens
            
            # Extraire et nettoyer les mots-cl√©s de la r√©ponse
            cleaned_keywords = response.choices[0].message.content.strip().split('\n')
            cleaned_keywords = [kw.strip() for kw in cleaned_keywords if kw.strip()]
            cleaned_keywords = list(dict.fromkeys(cleaned_keywords))

            current_batch_display.text("Mots-cl√©s nettoy√©s :")
            current_batch_display.text("\n".join(cleaned_keywords))
            
            return cleaned_keywords
        except Exception as api_error:
            error_message = str(api_error).lower()
            if "rate limit" in error_message:
                st.error(" Limite de l'API atteinte. Attendez quelques minutes avant de r√©essayer.")
            elif "insufficient_quota" in error_message or "billing" in error_message:
                st.error(" Cr√©dit OpenAI √©puis√© ! Vous devez recharger votre compte OpenAI.")
            elif "invalid_api_key" in error_message:
                st.error(" Cl√© API OpenAI invalide ! V√©rifiez votre fichier .env")
            else:
                st.error(f" Erreur OpenAI : {str(api_error)}")
            return []

    except Exception as e:
        st.error(f"Erreur : {str(e)}")
        return []

def create_zip_file(files_data):
    """
    Cr√©e un fichier ZIP contenant les fichiers nettoy√©s
    """
    zip_buffer = io.BytesIO()
    
    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
        for file_name, file_content in files_data:
            zip_file.writestr(file_name, file_content)
    
    zip_buffer.seek(0)
    return zip_buffer

def create_formatted_excel(merged_df):
    """
    Cr√©e un fichier Excel avec la mise en forme demand√©e
    """
    try:
        # Cr√©er un writer Excel
        output = io.BytesIO()
        writer = pd.ExcelWriter(output, engine='xlsxwriter')
        
        # Renommer la colonne si c'est un fichier unique
        if len(merged_df.columns) == 1:
            merged_df.columns = ['Collection 1']
        
        # √âcrire le DataFrame dans Excel
        merged_df.to_excel(writer, sheet_name='Keywords', index=False)
        
        # R√©cup√©rer le workbook et la worksheet
        workbook = writer.book
        worksheet = writer.sheets['Keywords']
        
        # D√©finir les formats
        header_format = workbook.add_format({
            'bg_color': '#00ffff',  # Cyan pour les en-t√™tes
            'bold': True,
            'align': 'center',
            'valign': 'vcenter'
        })
        
        first_row_format = workbook.add_format({
            'bg_color': '#ffff00',  # Jaune pour la premi√®re ligne
            'bold': True
        })
        
        # Appliquer le format aux en-t√™tes
        for col_num, value in enumerate(merged_df.columns.values):
            worksheet.write(0, col_num, value, header_format)
        
        # Appliquer le format jaune et gras √† la premi√®re ligne de donn√©es
        if not merged_df.empty:
            for col_num in range(len(merged_df.columns)):
                worksheet.write(1, col_num, merged_df.iloc[0, col_num], first_row_format)
        
        # Ajuster la largeur des colonnes
        for i, col in enumerate(merged_df.columns):
            max_length = max(
                merged_df[col].astype(str).apply(len).max(),
                len(str(col))
            )
            worksheet.set_column(i, i, max_length + 2)
        
        # Sauvegarder le fichier
        writer.close()
        
        # Pr√©parer le fichier pour le t√©l√©chargement
        output.seek(0)
        
        # Si c'est un seul fichier, utiliser le premier mot-cl√© comme nom
        if len(merged_df.columns) == 1:
            # R√©cup√©rer le premier mot-cl√©
            first_keyword = merged_df.iloc[0, 0] if not merged_df.empty else "keywords"
            # Nettoyer le nom pour un nom de fichier valide
            first_keyword = "".join(c for c in first_keyword if c.isalnum() or c in (' ', '-', '_')).strip()
            first_keyword = first_keyword.replace(' ', '_')
            filename = f"{first_keyword}_cleaned.xlsx"
        else:
            # Si multiple fichiers, garder le nom par d√©faut
            filename = "cleaned_keywords.xlsx"
            
        return output, filename
        
    except Exception as e:
        st.error(f"Erreur lors de la cr√©ation du fichier Excel : {str(e)}")
        return None, None

def merge_cleaned_files(cleaned_dataframes):
    """
    Fusionne tous les fichiers nettoy√©s en un seul DataFrame
    Chaque colonne est nomm√©e "Collection X"
    """
    # Cr√©er un DataFrame vide
    merged_df = pd.DataFrame()
    
    # Ajouter chaque colonne
    for idx, df in enumerate(cleaned_dataframes, 1):
        if not df.empty:
            # Renommer la colonne
            df.columns = [f'Collection {idx}']
            # Ajouter au DataFrame fusionn√©
            merged_df = pd.concat([merged_df, df], axis=1)
    
    return merged_df

def process_csv_file(file_path, current_file_display, current_batch_display, token_counter):
    try:
        # Lire le fichier CSV avec encodage UTF-8
        df = pd.read_csv(file_path, encoding='utf-8')
        
        # Phase 1 : Nettoyage de base
        current_file_display.markdown(f" Phase 1 : Nettoyage de base pour {Path(file_path).name}")
        
        # Convertir la colonne de mots-cl√©s en cha√Ænes de caract√®res
        df['Keyword'] = df['Keyword'].astype(str)
        
        # Nombre initial de mots-cl√©s
        initial_count = len(df)
        
        # Supprimer les doublons
        df = df.drop_duplicates(subset=['Keyword'], keep='first')
        duplicates_removed = initial_count - len(df)
        
        # Nettoyer les espaces uniquement, sans toucher aux accents
        df['Keyword'] = df['Keyword'].apply(lambda x: x.strip())
        
        # Supprimer les mots-cl√©s vides
        df = df[df['Keyword'].str.len() > 0]
        st.write(f"Apr√®s suppression mots-cl√©s vides : {len(df)}")
        
        # Supprimer les doublons exacts (sensible aux accents)
        df = df.drop_duplicates(subset=['Keyword'], keep='first')
        st.write(f"Apr√®s suppression doublons : {len(df)}")
        
        # Exclure les mots-cl√©s qui CONTIENNENT un mot ou groupe de mots de la liste d'exclusion
        def contains_excluded(keyword):
            # Convertir en minuscules pour la comparaison
            keyword_lower = keyword.lower()
            # S√©parer en mots
            keyword_words = set(keyword_lower.split())
            
            # V√©rifier chaque mot exclu
            for excluded_word in EXCLUDED_WORDS:
                excluded_word_lower = excluded_word.lower()
                # Si c'est un groupe de mots (ex: "pas cher")
                if ' ' in excluded_word_lower:
                    if excluded_word_lower in keyword_lower:
                        st.write(f"Mot-cl√© exclu : '{keyword}' (contient le groupe de mots '{excluded_word}')")
                        return True
                # Si c'est un mot unique (ex: "ou")
                else:
                    # V√©rifier si le mot exclu est un mot complet dans le mot-cl√©
                    if excluded_word_lower in keyword_words:
                        st.write(f"Mot-cl√© exclu : '{keyword}' (contient le mot exact '{excluded_word}')")
                        return True
            
            return False
        
        df = df[~df['Keyword'].apply(contains_excluded)]
        st.write(f"Apr√®s exclusion mots interdits : {len(df)}")
        
        # Supprimer les mots-cl√©s qui ont plus de 6 mots
        df['word_count'] = df['Keyword'].str.split().str.len()
        df = df[df['word_count'] <= 6]
        df = df.drop('word_count', axis=1)
        st.write(f"Apr√®s suppression mots-cl√©s > 6 mots : {len(df)}")
        
        # Afficher quelques exemples de mots-cl√©s restants
        if len(df) > 0:
            st.write("Exemples de mots-cl√©s conserv√©s :")
            st.write(df['Keyword'].head().tolist())
        else:
            st.write("Tous les mots-cl√©s ont √©t√© filtr√©s")
            
            # Afficher tous les mots-cl√©s avant filtrage pour comprendre
            st.write("Mots-cl√©s avant filtrage :")
            st.write(df['Keyword'].tolist())
        
        # Retourner uniquement les mots-cl√©s nettoy√©s
        return df['Keyword'].tolist()
    
    except Exception as e:
        st.error(f"Erreur lors du traitement du fichier {file_path}: {str(e)}")
        return [], None

def remove_similar_keywords(keywords, threshold=0.1):
    """
    Supprime les mots-cl√©s similaires en fonction du seuil de similarit√©
    """
    # Cr√©er un dictionnaire pour stocker les mots-cl√©s uniques
    unique_keywords = {}
    
    # It√©rer sur les mots-cl√©s
    for keyword in keywords:
        # Initialiser un indicateur pour savoir si le mot-cl√© est similaire
        is_similar = False
        
        # It√©rer sur les mots-cl√©s uniques
        for unique_keyword in unique_keywords:
            # Calculer la similarit√© entre les mots-cl√©s
            similarity = fuzz.ratio(keyword, unique_keyword)
            
            # Si la similarit√© est sup√©rieure au seuil, consid√©rer les mots-cl√©s comme similaires
            if similarity > threshold:
                is_similar = True
                break
        
        # Si le mot-cl√© n'est pas similaire, l'ajouter au dictionnaire
        if not is_similar:
            unique_keywords[keyword] = True
    
    # Retourner la liste des mots-cl√©s uniques
    return list(unique_keywords.keys())

def clean_keywords_phase1(df):
    """Phase 1 : Nettoyage algorithmique des mots-cl√©s"""
    try:
        # V√©rifier que les colonnes requises existent
        required_columns = ['Keyword', 'Volume']
        if not all(col in df.columns for col in required_columns):
            raise ValueError("Le fichier doit contenir les colonnes 'Keyword' et 'Volume'")
        
        st.write(f"Nombre initial de mots-cl√©s : {len(df)}")
        
        # Convertir la colonne Volume en num√©rique
        df['Volume'] = pd.to_numeric(df['Volume'], errors='coerce')
        st.write(f"Apr√®s conversion volume : {len(df)}")
        
        # Si le fichier d√©passe MAX_KEYWORDS lignes, on applique la limite
        if len(df) > MAX_KEYWORDS:
            # Trier par volume d√©croissant et garder les MAX_KEYWORDS premiers
            df = df.sort_values('Volume', ascending=False).head(MAX_KEYWORDS)
            st.write(f"Apr√®s limite MAX_KEYWORDS : {len(df)}")
        
        # Filtrer les volumes < MIN_VOLUME
        df = df[df['Volume'] >= MIN_VOLUME]
        st.write(f"Apr√®s filtre volume minimum : {len(df)}")
        
        # Nettoyer les espaces uniquement, sans toucher aux accents
        df['Keyword'] = df['Keyword'].astype(str).apply(lambda x: x.strip())
        
        # Supprimer les mots-cl√©s vides
        df = df[df['Keyword'].str.len() > 0]
        st.write(f"Apr√®s suppression mots-cl√©s vides : {len(df)}")
        
        # Supprimer les doublons exacts (sensible aux accents)
        df = df.drop_duplicates(subset=['Keyword'])
        st.write(f"Apr√®s suppression doublons : {len(df)}")
        
        # Exclure les mots-cl√©s qui CONTIENNENT un mot ou groupe de mots de la liste d'exclusion
        def contains_excluded(keyword):
            # Convertir en minuscules pour la comparaison
            keyword_lower = keyword.lower()
            # S√©parer en mots
            keyword_words = set(keyword_lower.split())
            
            # V√©rifier chaque mot exclu
            for excluded_word in EXCLUDED_WORDS:
                excluded_word_lower = excluded_word.lower()
                # Si c'est un groupe de mots (ex: "pas cher")
                if ' ' in excluded_word_lower:
                    if excluded_word_lower in keyword_lower:
                        st.write(f"Mot-cl√© exclu : '{keyword}' (contient le groupe de mots '{excluded_word}')")
                        return True
                # Si c'est un mot unique (ex: "ou")
                else:
                    # V√©rifier si le mot exclu est un mot complet dans le mot-cl√©
                    if excluded_word_lower in keyword_words:
                        st.write(f"Mot-cl√© exclu : '{keyword}' (contient le mot exact '{excluded_word}')")
                        return True
            
            return False
        
        df = df[~df['Keyword'].apply(contains_excluded)]
        st.write(f"Apr√®s exclusion mots interdits : {len(df)}")
        
        # Supprimer les mots-cl√©s qui ont plus de 6 mots
        df['word_count'] = df['Keyword'].str.split().str.len()
        df = df[df['word_count'] <= 6]
        df = df.drop('word_count', axis=1)
        st.write(f"Apr√®s suppression mots-cl√©s > 6 mots : {len(df)}")
        
        # Afficher quelques exemples de mots-cl√©s restants
        if len(df) > 0:
            st.write("Exemples de mots-cl√©s conserv√©s :")
            st.write(df['Keyword'].head().tolist())
        else:
            st.write("Tous les mots-cl√©s ont √©t√© filtr√©s")
            
            # Afficher tous les mots-cl√©s avant filtrage pour comprendre
            st.write("Mots-cl√©s avant filtrage :")
            st.write(df['Keyword'].tolist())
        
        # Retourner uniquement les mots-cl√©s nettoy√©s
        return df['Keyword'].tolist()
    
    except Exception as e:
        raise Exception(f"Erreur lors du nettoyage phase 1 : {str(e)}")

def main():
    st.title(" Nettoyeur de Mots-Cl√©s SEO - Etape 1")
    
    # Ajouter le CSS pour l'effet pulse et les stats
    st.markdown("""
        <style>
        .stApp {
            background-color: #f0f2f6;
        }
        
        .stButton button {
            background-color: #4CAF50 !important;
            color: white !important;
            border: none !important;
            padding: 0.5rem 1rem !important;
            border-radius: 0.5rem !important;
            margin-top: 1rem !important;
            width: 100% !important;
        }
        
        .stDownloadButton button {
            background-color: #2196F3 !important;
            color: white !important;
            border: none !important;
            padding: 0.5rem 1rem !important;
            border-radius: 0.5rem !important;
            margin: 0.5rem 0 !important;
            width: 100% !important;
        }
        
        @keyframes pulse {
            0% { box-shadow: 0 0 0 0 rgba(76, 175, 80, 0.7); }
            70% { box-shadow: 0 0 0 10px rgba(76, 175, 80, 0); }
            100% { box-shadow: 0 0 0 0 rgba(76, 175, 80, 0); }
        }
        
        .combined-button button {
            animation: pulse 2s infinite !important;
            background-color: #4CAF50 !important;
            color: white !important;
            border: none !important;
            padding: 1.2rem !important;
            border-radius: 0.5rem !important;
            margin: 2rem auto !important;
            width: 100% !important;
            font-size: 1.3em !important;
            font-weight: 900 !important;
            text-transform: uppercase !important;
            letter-spacing: 0.5px !important;
        }
        
        .stats-container {
            background-color: white;
            padding: 1.5rem;
            border-radius: 0.5rem;
            margin: 0.5rem 0;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            text-align: center;
        }
        
        .stat-icon {
            font-size: 2em;
            color: #4CAF50;
            display: block;
            margin-bottom: 0.5rem;
        }
        
        .stat-label {
            font-size: 0.9em;
            color: #666;
            margin-bottom: 0.5rem;
            font-weight: 500;
        }
        
        .stat-value {
            font-weight: 700;
            color: #1e3d59;
            font-size: 1.1em;
        }
        
        .progress-container {
            margin: 1rem 0;
        }
        
        .stProgress .st-bo {
            background-color: #4CAF50;
        }
        </style>
    """, unsafe_allow_html=True)
    
    # Initialiser les variables de session si elles n'existent pas
    if 'processed_files' not in st.session_state:
        st.session_state.processed_files = []
    if 'all_dataframes' not in st.session_state:
        st.session_state.all_dataframes = []
    if 'token_counter' not in st.session_state:
        st.session_state.token_counter = {
            'input_tokens': 0,
            'output_tokens': 0,
            'total_tokens': 0
        }
    
    # Initialiser la liste des DataFrames nettoy√©s dans la session
    if 'cleaned_dataframes' not in st.session_state:
        st.session_state.cleaned_dataframes = []
    
    # Zone de d√©p√¥t des fichiers
    uploaded_files = st.file_uploader(" D√©posez vos fichiers Excel ou CSV", 
                                    type=['xlsx', 'csv'], 
                                    accept_multiple_files=True)

    # Cr√©er des colonnes pour les boutons
    col1, col2 = st.columns([0.6, 0.4])
    
    with col1:
        if st.button(" Nettoyer les Mots-Cl√©s", type="primary"):
            if not uploaded_files:
                st.error("Veuillez d'abord d√©poser des fichiers.")
                return
            
            # R√©initialiser la session
            st.session_state.cleaned_dataframes = []
            st.session_state.token_counter = {
                'input_tokens': 0,
                'output_tokens': 0,
                'total_tokens': 0
            }
                
            # Cr√©er les conteneurs pour l'affichage du traitement
            stats_cols = st.columns(4)
            with stats_cols[0]:
                st.markdown("""
                    <div class='stats-container'>
                        <span class='stat-icon'>üìä</span>
                        <div class='stat-label'>Fichier en cours</div>
                        <div id='file-progress' class='stat-value'></div>
                    </div>
                """, unsafe_allow_html=True)
                file_progress_text = st.empty()
            
            with stats_cols[1]:
                st.markdown("""
                    <div class='stats-container'>
                        <span class='stat-icon'>üîÑ</span>
                        <div class='stat-label'>Lot en cours</div>
                        <div id='batch-progress' class='stat-value'></div>
                    </div>
                """, unsafe_allow_html=True)
                current_batch_container = st.empty()
            
            with stats_cols[2]:
                st.markdown("""
                    <div class='stats-container'>
                        <span class='stat-icon'>üí∏</span>
                        <div class='stat-label'>Co√ªt total</div>
                        <div id='cost-estimate' class='stat-value'></div>
                    </div>
                """, unsafe_allow_html=True)
                cost_container = st.empty()
            
            with stats_cols[3]:
                st.markdown("""
                    <div class='stats-container'>
                        <span class='stat-icon'>üìà</span>
                        <div class='stat-label'>Progression</div>
                        <div id='progress' class='stat-value'></div>
                    </div>
                """, unsafe_allow_html=True)
                progress_container = st.empty()
            
            # Conteneur pour la barre de progression
            st.markdown("<div class='progress-container'>", unsafe_allow_html=True)
            progress_bar = progress_container.progress(0)
            st.markdown("</div>", unsafe_allow_html=True)
            
            # Traiter chaque fichier
            progress_bar = progress_container.progress(0)
            total_files = len(uploaded_files)
            
            # Liste pour stocker les r√©sultats de la phase 1
            phase1_results = []
            
            # PHASE 1 : Traiter tous les fichiers avec le nettoyage algorithmique
            for i, file in enumerate(uploaded_files):
                # Afficher le fichier en cours
                file_progress_text.text(f"Fichier en cours : {i + 1}/{total_files}")
                
                try:
                    # Lire le fichier
                    if file.name.endswith('.csv'):
                        df = pd.read_csv(file, encoding='utf-8')
                    else:
                        df = pd.read_excel(file)
                    
                    # Phase 1 : Nettoyage algorithmique
                    cleaned_keywords_phase1 = clean_keywords_phase1(df)
                    
                    # Si aucun mot-cl√© ne passe la phase 1, passer au fichier suivant
                    if not cleaned_keywords_phase1:
                        st.warning(f"Aucun mot-cl√© valide trouv√© dans {file.name} apr√®s la phase 1")
                        continue
                        
                    # Stocker les r√©sultats de la phase 1
                    phase1_results.append({
                        'file_name': file.name,
                        'keywords': cleaned_keywords_phase1
                    })
                    
                except Exception as e:
                    st.error(f"Erreur lors du traitement du fichier {file.name}: {str(e)}")
                    continue
                
                # Mettre √† jour la barre de progression
                progress_bar.progress((i + 1) / total_files)
            
            # PHASE 2 : Traiter les r√©sultats de la phase 1 avec GPT
            all_cleaned_dfs = []
            for i, result in enumerate(phase1_results):
                file_progress_text.text(f"Fichier en cours : {i + 1}/{len(phase1_results)}")
                
                try:
                    # Phase 2 : Nettoyage GPT
                    cleaned_keywords = []
                    keywords_phase1 = result['keywords']
                    total_batches = len(keywords_phase1) // BATCH_SIZE + (1 if len(keywords_phase1) % BATCH_SIZE > 0 else 0)
                    
                    # Barre de progression pour les lots
                    batch_progress = st.progress(0)
                    
                    # Traiter les mots-cl√©s par lots
                    for batch_num, start_idx in enumerate(range(0, len(keywords_phase1), BATCH_SIZE)):
                        # Mettre √† jour la progression des lots
                        current_batch_container.text(f"Lot en cours : {batch_num + 1}/{total_batches}")
                        
                        # Pr√©parer le lot
                        end_idx = min(start_idx + BATCH_SIZE, len(keywords_phase1))
                        current_batch = keywords_phase1[start_idx:end_idx]
                        
                        # Nettoyer le lot avec GPT
                        cleaned_batch = clean_keywords_with_gpt(current_batch, current_batch_container, st.session_state.token_counter)
                        if cleaned_batch:
                            cleaned_keywords.extend(cleaned_batch)
                        
                        # Mettre √† jour la progression des lots
                        batch_progress.progress((batch_num + 1) / total_batches)
                        
                        # Petite pause entre les lots pour √©viter les limites de l'API
                        time.sleep(0.5)
                    
                    # Cr√©er le DataFrame final pour ce fichier
                    if cleaned_keywords:
                        cleaned_df = pd.DataFrame({
                            'Keyword': cleaned_keywords,
                            'Volume': [0] * len(cleaned_keywords)  # Volume par d√©faut
                        })
                        all_cleaned_dfs.append(cleaned_df)
                        
                        # Cr√©er et t√©l√©charger le fichier Excel
                        excel_data, filename = create_formatted_excel(cleaned_df)
                        
                        # Ajouter le bouton de t√©l√©chargement
                        with col2:
                            st.download_button(
                                label=f" T√©l√©charger {result['file_name'].replace('.csv', '_clean.xlsx')}",
                                data=excel_data,
                                file_name=result['file_name'].replace('.csv', '_clean.xlsx'),
                                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                            )
                    
                except Exception as e:
                    st.error(f"Erreur lors du traitement GPT du fichier {result['file_name']}: {str(e)}")
                    continue
                
                # Mettre √† jour la barre de progression
                progress_bar.progress((i + 1) / len(phase1_results))
            
            # Cr√©er un DataFrame combin√© avec toutes les collections
            if len(all_cleaned_dfs) > 1:
                # Cr√©er un dictionnaire pour stocker les collections
                collections_dict = {}
                
                # Traiter chaque DataFrame s√©par√©ment
                for i, df in enumerate(all_cleaned_dfs, 1):
                    collections_dict[f'Collection {i}'] = df['Keyword'].tolist()
                
                # Trouver la longueur maximale
                max_len = max(len(keywords) for keywords in collections_dict.values())
                
                # Cr√©er un nouveau DataFrame avec des colonnes align√©es
                combined_data = {}
                for collection_name, keywords in collections_dict.items():
                    # √âtendre la liste avec None pour avoir la m√™me longueur
                    padded_keywords = keywords + [None] * (max_len - len(keywords))
                    combined_data[collection_name] = padded_keywords
                
                combined_df = pd.DataFrame(combined_data)
                
                # Cr√©er le fichier Excel avec mise en forme
                excel_buffer = io.BytesIO()
                with pd.ExcelWriter(excel_buffer, engine='xlsxwriter') as writer:
                    # √âcrire le DataFrame principal
                    combined_df.to_excel(writer, sheet_name='Keywords', index=False)
                    
                    # R√©cup√©rer le workbook et la worksheet
                    workbook = writer.book
                    worksheet = writer.sheets['Keywords']
                    
                    # D√©finir les formats
                    header_format = workbook.add_format({
                        'bg_color': '#00ffff',  # Cyan pour les en-t√™tes
                        'bold': True,
                        'align': 'center',
                        'valign': 'vcenter'
                    })
                    
                    first_row_format = workbook.add_format({
                        'bg_color': '#ffff00',  # Jaune pour la premi√®re ligne
                        'bold': True
                    })
                    
                    # Appliquer le format aux en-t√™tes
                    for col_num, value in enumerate(combined_df.columns.values):
                        worksheet.write(0, col_num, value, header_format)
                    
                    # Appliquer le format jaune et gras √† la premi√®re ligne de donn√©es
                    if not combined_df.empty:
                        for col_num in range(len(combined_df.columns)):
                            if pd.notna(combined_df.iloc[0, col_num]):
                                worksheet.write(1, col_num, combined_df.iloc[0, col_num], first_row_format)
                    
                    # Ajuster la largeur des colonnes
                    for i, col in enumerate(combined_df.columns):
                        max_length = max(
                            combined_df[col].astype(str).apply(len).max(),
                            len(str(col))
                        )
                        worksheet.set_column(i, i, max_length + 2)
                
                excel_buffer.seek(0)
                excel_data = excel_buffer.getvalue()
                
                # Ajouter un espacement avant le bouton combin√©
                with col2:
                    st.markdown("<br><br>", unsafe_allow_html=True)
                    st.markdown("<div class='combined-button'>", unsafe_allow_html=True)
                    st.download_button(
                        label=" T√âL√âCHARGER TOUTES LES COLLECTIONS COMBIN√âES ",
                        data=excel_data,
                        file_name="all_collections.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                        key="download_all"
                    )
                    st.markdown("</div>", unsafe_allow_html=True)
            
            # Effacer les conteneurs de progression une fois termin√©
            progress_container.empty()
            file_progress_text.empty()
            current_batch_container.empty()
            
            # Afficher les statistiques d'utilisation
            st.markdown("### Statistiques d'utilisation")
            token_counter = st.session_state.token_counter
            input_cost = (token_counter['input_tokens'] / 1000) * 0.0015
            output_cost = (token_counter['output_tokens'] / 1000) * 0.002
            total_cost = input_cost + output_cost
            
            cost_container.markdown(f"<div class='stats-container'>\n    <span class='stat-icon'>üí∏</span>\n    <div class='stat-label'>Co√ªt total</div>\n    <div class='stat-value'>${total_cost:.4f}</div>\n</div>", unsafe_allow_html=True)
            
            st.markdown(f"""
            - Tokens en entr√©e : {token_counter['input_tokens']:,} (${input_cost:.4f})
            - Tokens en sortie : {token_counter['output_tokens']:,} (${output_cost:.4f})
            - Total tokens : {token_counter['total_tokens']:,}
            """)
            
            st.success("Traitement termin√© !")
            
if __name__ == "__main__":
    main()